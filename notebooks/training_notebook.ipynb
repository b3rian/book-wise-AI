{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras_nlp as keras_hub \n",
    "from tensorflow.keras import layers\n",
    "from configs.configss import config\n",
    "\n",
    "# constants\n",
    "SEQ_LEN = 128 # Maximum sequence length for tokenization\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Auto-tune dataset performance\n",
    "\n",
    "def load_and_clean_lines(file_path, min_words=3, max_words=250):\n",
    "    \"\"\"\n",
    "    Load and clean lines from a given text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        min_words (int): Minimum number of words per line.\n",
    "        max_words (int): Maximum number of words per line.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned text lines.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    return [\n",
    "        line.strip()\n",
    "        for line in lines\n",
    "        if line.strip() and min_words < len(line.strip().split()) < max_words\n",
    "    ]\n",
    "\n",
    "def write_cleaned_lines(output_path, lines):\n",
    "    \"\"\"\n",
    "    Write cleaned lines to a text file.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): Destination file path.\n",
    "        lines (list): List of cleaned strings.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "def build_tokenizer(vocab_path, seq_len=128):\n",
    "    \"\"\"\n",
    "    Build WordPiece tokenizer and packing layer.\n",
    "\n",
    "    Args:\n",
    "        vocab_path (str): Path to vocabulary file.\n",
    "        seq_len (int): Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A WordPiece tokenizer.\n",
    "        start_packer: A layer that adds a start token and pads/truncates to `seq_len`.\n",
    "    \"\"\"\n",
    "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n",
    "\n",
    "     # Add special tokens\n",
    "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\"]\n",
    "    vocab = reserved_tokens + vocab\n",
    "\n",
    "    tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "        vocabulary=vocab,\n",
    "        sequence_length=seq_len,\n",
    "        lowercase=False,\n",
    "    )\n",
    "    \n",
    "    # Create a StartEndPacker layer to handle start token and padding\n",
    "    start_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=seq_len,\n",
    "        start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "    )\n",
    "\n",
    "    return tokenizer, start_packer\n",
    "\n",
    "def preprocess_fn(text, tokenizer, start_packer):\n",
    "    \"\"\"\n",
    "    Tokenizes and packs input text for training.\n",
    "\n",
    "    Args:\n",
    "        text (tf.Tensor): Raw text input.\n",
    "        tokenizer: WordPiece tokenizer.\n",
    "        start_packer: Layer to pack and add [BOS] token.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (input_tensor, label_tensor)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    inputs = start_packer(tokens)\n",
    "    labels = tokens  # Model learns to predict next tokens\n",
    "    return inputs, labels\n",
    "\n",
    "def create_dataset(file_path, tokenizer, start_packer, is_training=False):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset pipeline.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the cleaned dataset file.\n",
    "        tokenizer: Tokenizer instance.\n",
    "        start_packer: Token packer layer.\n",
    "        is_training (bool): Whether the dataset is used for training.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Preprocessed batched dataset.\n",
    "    \"\"\"\n",
    "    ds = tf.data.TextLineDataset(file_path) # Load text lines from file\n",
    "\n",
    "    if is_training:\n",
    "        ds = ds.cache().shuffle(10000) # Shuffle and cache dataset for training\n",
    "\n",
    "    ds = (\n",
    "        ds.map(lambda x: preprocess_fn(x, tokenizer, start_packer), num_parallel_calls=AUTOTUNE)\n",
    "          .batch(BATCH_SIZE) # Batch the dataset\n",
    "          .prefetch(AUTOTUNE) # Prefetch for performance\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def main():\n",
    "    # Base directory containing raw and vocab files\n",
    "    raw_data_dir = \"/content/simplebooks_data/simplebooks/simplebooks-92-raw\"\n",
    "    clean_data_dir = \"/content/simplebooks_clean\"\n",
    "    os.makedirs(clean_data_dir, exist_ok=True)\n",
    "    vocab_path: \"/content/simplebooks_data/simplebooks/simplebooks-92/train.vocab\"\n",
    "\n",
    "    # Define file paths\n",
    "    train_raw = os.path.join(raw_data_dir, \"train.txt\")\n",
    "    valid_raw = os.path.join(raw_data_dir, \"valid.txt\")\n",
    "    test_raw = os.path.join(raw_data_dir, \"test.txt\")\n",
    "\n",
    "    # Cleaned file paths\n",
    "    train_clean = os.path.join(clean_data_dir, \"train_clean.txt\")\n",
    "    valid_clean = os.path.join(clean_data_dir, \"valid_clean.txt\")\n",
    "    test_clean = os.path.join(clean_data_dir, \"test_clean.txt\")\n",
    "\n",
    "    # Clean and save text\n",
    "    write_cleaned_lines(train_clean, load_and_clean_lines(train_raw))\n",
    "    write_cleaned_lines(valid_clean, load_and_clean_lines(valid_raw))\n",
    "    write_cleaned_lines(test_clean, load_and_clean_lines(test_raw))\n",
    "\n",
    "    # Build tokenizer and packer\n",
    "    tokenizer, start_packer = build_tokenizer(vocab_path, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds = create_dataset(train_clean, tokenizer, start_packer, is_training=True)\n",
    "    val_ds = create_dataset(valid_clean, tokenizer, start_packer)\n",
    "    test_ds = create_dataset(test_clean, tokenizer, start_packer)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, tokenizer, start_packer \n",
    "\n",
    "# Run the preprocessing pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    train_ds, val_ds, test_ds, tokenizer, start_packer = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
