{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline for Cleaning and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras_nlp as keras_hub \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# constants\n",
    "SEQ_LEN = 128 # Maximum sequence length for tokenization\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Auto-tune dataset performance\n",
    "\n",
    "def load_and_clean_lines(file_path, min_words=3, max_words=250):\n",
    "    \"\"\"\n",
    "    Load and clean lines from a given text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        min_words (int): Minimum number of words per line.\n",
    "        max_words (int): Maximum number of words per line.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned text lines.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    return [\n",
    "        line.strip()\n",
    "        for line in lines\n",
    "        if line.strip() and min_words < len(line.strip().split()) < max_words\n",
    "    ]\n",
    "\n",
    "def write_cleaned_lines(output_path, lines):\n",
    "    \"\"\"\n",
    "    Write cleaned lines to a text file.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): Destination file path.\n",
    "        lines (list): List of cleaned strings.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "def build_tokenizer(vocab_path, seq_len=128):\n",
    "    \"\"\"\n",
    "    Build WordPiece tokenizer and packing layer.\n",
    "\n",
    "    Args:\n",
    "        vocab_path (str): Path to vocabulary file.\n",
    "        seq_len (int): Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A WordPiece tokenizer.\n",
    "        start_packer: A layer that adds a start token and pads/truncates to `seq_len`.\n",
    "    \"\"\"\n",
    "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n",
    "\n",
    "     # Add special tokens\n",
    "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\"]\n",
    "    vocab = reserved_tokens + vocab\n",
    "\n",
    "    tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "        vocabulary=vocab,\n",
    "        sequence_length=seq_len,\n",
    "        lowercase=False,\n",
    "    )\n",
    "    \n",
    "    # Create a StartEndPacker layer to handle start token and padding\n",
    "    start_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=seq_len,\n",
    "        start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "    )\n",
    "\n",
    "    return tokenizer, start_packer\n",
    "\n",
    "def preprocess_fn(text, tokenizer, start_packer):\n",
    "    \"\"\"\n",
    "    Tokenizes and packs input text for training.\n",
    "\n",
    "    Args:\n",
    "        text (tf.Tensor): Raw text input.\n",
    "        tokenizer: WordPiece tokenizer.\n",
    "        start_packer: Layer to pack and add [BOS] token.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (input_tensor, label_tensor)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    inputs = start_packer(tokens)\n",
    "    labels = tokens  # Model learns to predict next tokens\n",
    "    return inputs, labels\n",
    "\n",
    "def create_dataset(file_path, tokenizer, start_packer, is_training=False):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset pipeline.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the cleaned dataset file.\n",
    "        tokenizer: Tokenizer instance.\n",
    "        start_packer: Token packer layer.\n",
    "        is_training (bool): Whether the dataset is used for training.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Preprocessed batched dataset.\n",
    "    \"\"\"\n",
    "    ds = tf.data.TextLineDataset(file_path) # Load text lines from file\n",
    "\n",
    "    if is_training:\n",
    "        ds = ds.cache().shuffle(10000) # Shuffle and cache dataset for training\n",
    "\n",
    "    ds = (\n",
    "        ds.map(lambda x: preprocess_fn(x, tokenizer, start_packer), num_parallel_calls=AUTOTUNE)\n",
    "          .batch(BATCH_SIZE) # Batch the dataset\n",
    "          .prefetch(AUTOTUNE) # Prefetch for performance\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def main():\n",
    "    # Base directory containing raw and vocab files\n",
    "    raw_data_dir = \"/content/simplebooks_data/simplebooks/simplebooks-92-raw\"\n",
    "    clean_data_dir = \"/content/simplebooks_clean\"\n",
    "    os.makedirs(clean_data_dir, exist_ok=True)\n",
    "    vocab_path: \"/content/simplebooks_data/simplebooks/simplebooks-92/train.vocab\"\n",
    "\n",
    "    # Define file paths\n",
    "    train_raw = os.path.join(raw_data_dir, \"train.txt\")\n",
    "    valid_raw = os.path.join(raw_data_dir, \"valid.txt\")\n",
    "    test_raw = os.path.join(raw_data_dir, \"test.txt\")\n",
    "\n",
    "    # Cleaned file paths\n",
    "    train_clean = os.path.join(clean_data_dir, \"train_clean.txt\")\n",
    "    valid_clean = os.path.join(clean_data_dir, \"valid_clean.txt\")\n",
    "    test_clean = os.path.join(clean_data_dir, \"test_clean.txt\")\n",
    "\n",
    "    # Clean and save text\n",
    "    write_cleaned_lines(train_clean, load_and_clean_lines(train_raw))\n",
    "    write_cleaned_lines(valid_clean, load_and_clean_lines(valid_raw))\n",
    "    write_cleaned_lines(test_clean, load_and_clean_lines(test_raw))\n",
    "\n",
    "    # Build tokenizer and packer\n",
    "    tokenizer, start_packer = build_tokenizer(vocab_path, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds = create_dataset(train_clean, tokenizer, start_packer, is_training=True)\n",
    "    val_ds = create_dataset(valid_clean, tokenizer, start_packer)\n",
    "    test_ds = create_dataset(test_clean, tokenizer, start_packer)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, tokenizer, start_packer \n",
    "\n",
    "# Run the preprocessing pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    train_ds, val_ds, test_ds, tokenizer, start_packer = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position and Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Combines token and positional embeddings.\n",
    "\n",
    "    This layer learns:\n",
    "    - An embedding vector for each token in the vocabulary.\n",
    "    - An embedding vector for each position in the input sequence.\n",
    "    \n",
    "    The final embedding is a sum of the token embedding and the positional embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the token and position embedding layers.\n",
    "\n",
    "        Args:\n",
    "            maxlen (int): Maximum length of the input sequences.\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embed_dim (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Applies token and positional embeddings to the input.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "        \"\"\"\n",
    "        seq_len = ops.shape(x)[-1] # Get the sequence length from the input tensor shape\n",
    "\n",
    "        # Create position indices [0, 1, 2, ..., sequence_length - 1]\n",
    "        positions = ops.arange(0, seq_len)\n",
    "\n",
    "        # Look up position embeddings\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "\n",
    "        # Look up token embeddings\n",
    "        token_embeddings = self.token_emb(x)\n",
    "\n",
    "        # Combine both\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, ops\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Generates a causal attention mask to prevent attention to future tokens.\n",
    "\n",
    "    This is used in decoder-only architectures like GPT, where tokens should\n",
    "    only attend to previous or current positions (not future ones).\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int or Tensor): Number of sequences in a batch.\n",
    "    - n_dest (int): Number of destination positions (usually equal to sequence length).\n",
    "    - n_src (int): Number of source positions (same as n_dest for self-attention).\n",
    "    - dtype (tf.DType or str): The data type of the output mask, e.g., 'bool' or 'float32'.\n",
    "\n",
    "    Returns:\n",
    "    - tf.Tensor: A lower triangular mask of shape (batch_size, n_dest, n_src)\n",
    "    \"\"\"\n",
    "    # Create destination and source position indices\n",
    "    i = ops.arange(n_dest)[:, None]  # Shape: (n_dest, 1)\n",
    "    j = ops.arange(n_src)            # Shape: (n_src,)\n",
    "\n",
    "    # Compute lower triangular matrix (causal mask)\n",
    "    mask_matrix = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(mask_matrix, dtype)  # Convert boolean mask to specified dtype\n",
    "\n",
    "    # Reshape to add batch dimension\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "\n",
    "    # Tile the mask to match the batch size\n",
    "    mult = ops.concatenate([\n",
    "        ops.expand_dims(batch_size, -1),  # Shape: [1]\n",
    "        ops.convert_to_tensor([1, 1])     # Shape: [2]\n",
    "    ], axis=0)\n",
    "    \n",
    "    return ops.tile(mask, mult)  # Final shape: (batch_size, n_dest, n_src)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    A single transformer decoder block implementing:\n",
    "    - Causal self-attention (no lookahead)\n",
    "    - Feedforward neural network (FFN)\n",
    "    - Residual connections\n",
    "    - Layer normalization\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, name=None):\n",
    "        \"\"\"\n",
    "        Initializes the transformer block.\n",
    "\n",
    "        Parameters:\n",
    "        - embed_dim (int): Dimension of the token embeddings.\n",
    "        - num_heads (int): Number of attention heads.\n",
    "        - ff_dim (int): Hidden dimension of the feedforward network.\n",
    "        - rate (float): Dropout rate.\n",
    "        - name (str): Optional name for the layer.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name) # Initialize the base Layer class\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),  # Position-wise feedforward\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Executes the forward pass of the transformer block.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs (tf.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        - training (bool): Whether the call is in training mode (enables dropout)\n",
    "\n",
    "        Returns:\n",
    "        - tf.Tensor: Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        # Extract batch size and sequence length\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "\n",
    "        # Generate causal mask to block attention to future tokens\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size=batch_size,\n",
    "            n_dest=seq_len,\n",
    "            n_src=seq_len,\n",
    "            dtype=\"bool\"\n",
    "        )\n",
    "        # Apply causal multi-head self-attention\n",
    "        attention_output = self.att(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            training=training\n",
    "        )\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attention_output)  # Residual + Norm\n",
    "\n",
    "        # Feedforward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)  # Residual + Norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 128\n",
    "vocab_size = 98308\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "ff_dim = 1024\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers, ops\n",
    "\n",
    "def create_model(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim):\n",
    "    \"\"\"\n",
    "    Builds and compiles a simple transformer-based language model.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token and position embeddings.\n",
    "        num_heads (int): Number of attention heads in the transformer block.\n",
    "        feed_forward_dim (int): Dimension of the feed-forward network.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    # Input layer expecting integer token IDs\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\", name=\"input_tokens\")\n",
    "\n",
    "    # Token and position embeddings\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    # Transformer block with causal masking\n",
    "    # Stack Transformer blocks dynamically\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "    # Final dense layer maps to vocabulary size for language modeling\n",
    "    logits = layers.Dense(vocab_size, name=\"output_logits\")(x)\n",
    "\n",
    "    # Define model with both logits and intermediate embeddings as output (for optional use)\n",
    "    model = keras.Model(inputs=inputs, outputs=[logits, x], name=\"transformer_decoder\")\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
