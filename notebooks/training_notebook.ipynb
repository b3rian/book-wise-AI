{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline for Cleaning and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras_nlp as keras_hub \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# constants\n",
    "SEQ_LEN = 128 # Maximum sequence length for tokenization\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Auto-tune dataset performance\n",
    "\n",
    "def load_and_clean_lines(file_path, min_words=3, max_words=250):\n",
    "    \"\"\"\n",
    "    Load and clean lines from a given text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        min_words (int): Minimum number of words per line.\n",
    "        max_words (int): Maximum number of words per line.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned text lines.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    return [\n",
    "        line.strip()\n",
    "        for line in lines\n",
    "        if line.strip() and min_words < len(line.strip().split()) < max_words\n",
    "    ]\n",
    "\n",
    "def write_cleaned_lines(output_path, lines):\n",
    "    \"\"\"\n",
    "    Write cleaned lines to a text file.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): Destination file path.\n",
    "        lines (list): List of cleaned strings.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "def build_tokenizer(vocab_path, seq_len=128):\n",
    "    \"\"\"\n",
    "    Build WordPiece tokenizer and packing layer.\n",
    "\n",
    "    Args:\n",
    "        vocab_path (str): Path to vocabulary file.\n",
    "        seq_len (int): Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A WordPiece tokenizer.\n",
    "        start_packer: A layer that adds a start token and pads/truncates to `seq_len`.\n",
    "    \"\"\"\n",
    "    with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n",
    "\n",
    "     # Add special tokens\n",
    "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\"]\n",
    "    vocab = reserved_tokens + vocab\n",
    "\n",
    "    tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "        vocabulary=vocab,\n",
    "        sequence_length=seq_len,\n",
    "        lowercase=False,\n",
    "    )\n",
    "    \n",
    "    # Create a StartEndPacker layer to handle start token and padding\n",
    "    start_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=seq_len,\n",
    "        start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
    "    )\n",
    "\n",
    "    return tokenizer, start_packer\n",
    "\n",
    "def preprocess_fn(text, tokenizer, start_packer):\n",
    "    \"\"\"\n",
    "    Tokenizes and packs input text for training.\n",
    "\n",
    "    Args:\n",
    "        text (tf.Tensor): Raw text input.\n",
    "        tokenizer: WordPiece tokenizer.\n",
    "        start_packer: Layer to pack and add [BOS] token.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (input_tensor, label_tensor)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    inputs = start_packer(tokens)\n",
    "    labels = tokens  # Model learns to predict next tokens\n",
    "    return inputs, labels\n",
    "\n",
    "def create_dataset(file_path, tokenizer, start_packer, is_training=False):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset pipeline.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the cleaned dataset file.\n",
    "        tokenizer: Tokenizer instance.\n",
    "        start_packer: Token packer layer.\n",
    "        is_training (bool): Whether the dataset is used for training.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Preprocessed batched dataset.\n",
    "    \"\"\"\n",
    "    ds = tf.data.TextLineDataset(file_path) # Load text lines from file\n",
    "\n",
    "    if is_training:\n",
    "        ds = ds.cache().shuffle(10000) # Shuffle and cache dataset for training\n",
    "\n",
    "    ds = (\n",
    "        ds.map(lambda x: preprocess_fn(x, tokenizer, start_packer), num_parallel_calls=AUTOTUNE)\n",
    "          .batch(BATCH_SIZE) # Batch the dataset\n",
    "          .prefetch(AUTOTUNE) # Prefetch for performance\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def main():\n",
    "    # Base directory containing raw and vocab files\n",
    "    raw_data_dir = \"/content/simplebooks_data/simplebooks/simplebooks-92-raw\"\n",
    "    clean_data_dir = \"/content/simplebooks_clean\"\n",
    "    os.makedirs(clean_data_dir, exist_ok=True)\n",
    "    vocab_path: \"/content/simplebooks_data/simplebooks/simplebooks-92/train.vocab\"\n",
    "\n",
    "    # Define file paths\n",
    "    train_raw = os.path.join(raw_data_dir, \"train.txt\")\n",
    "    valid_raw = os.path.join(raw_data_dir, \"valid.txt\")\n",
    "    test_raw = os.path.join(raw_data_dir, \"test.txt\")\n",
    "\n",
    "    # Cleaned file paths\n",
    "    train_clean = os.path.join(clean_data_dir, \"train_clean.txt\")\n",
    "    valid_clean = os.path.join(clean_data_dir, \"valid_clean.txt\")\n",
    "    test_clean = os.path.join(clean_data_dir, \"test_clean.txt\")\n",
    "\n",
    "    # Clean and save text\n",
    "    write_cleaned_lines(train_clean, load_and_clean_lines(train_raw))\n",
    "    write_cleaned_lines(valid_clean, load_and_clean_lines(valid_raw))\n",
    "    write_cleaned_lines(test_clean, load_and_clean_lines(test_raw))\n",
    "\n",
    "    # Build tokenizer and packer\n",
    "    tokenizer, start_packer = build_tokenizer(vocab_path, seq_len=SEQ_LEN)\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds = create_dataset(train_clean, tokenizer, start_packer, is_training=True)\n",
    "    val_ds = create_dataset(valid_clean, tokenizer, start_packer)\n",
    "    test_ds = create_dataset(test_clean, tokenizer, start_packer)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, tokenizer, start_packer \n",
    "\n",
    "# Run the preprocessing pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    train_ds, val_ds, test_ds, tokenizer, start_packer = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position and Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Combines token and positional embeddings.\n",
    "\n",
    "    This layer learns:\n",
    "    - An embedding vector for each token in the vocabulary.\n",
    "    - An embedding vector for each position in the input sequence.\n",
    "    \n",
    "    The final embedding is a sum of the token embedding and the positional embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the token and position embedding layers.\n",
    "\n",
    "        Args:\n",
    "            maxlen (int): Maximum length of the input sequences.\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embed_dim (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Applies token and positional embeddings to the input.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape (batch_size, sequence_length, embed_dim).\n",
    "        \"\"\"\n",
    "        seq_len = ops.shape(x)[-1] # Get the sequence length from the input tensor shape\n",
    "\n",
    "        # Create position indices [0, 1, 2, ..., sequence_length - 1]\n",
    "        positions = ops.arange(0, seq_len)\n",
    "\n",
    "        # Look up position embeddings\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "\n",
    "        # Look up token embeddings\n",
    "        token_embeddings = self.token_emb(x)\n",
    "\n",
    "        # Combine both\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, ops\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Generates a causal attention mask to prevent attention to future tokens.\n",
    "\n",
    "    This is used in decoder-only architectures like GPT, where tokens should\n",
    "    only attend to previous or current positions (not future ones).\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int or Tensor): Number of sequences in a batch.\n",
    "    - n_dest (int): Number of destination positions (usually equal to sequence length).\n",
    "    - n_src (int): Number of source positions (same as n_dest for self-attention).\n",
    "    - dtype (tf.DType or str): The data type of the output mask, e.g., 'bool' or 'float32'.\n",
    "\n",
    "    Returns:\n",
    "    - tf.Tensor: A lower triangular mask of shape (batch_size, n_dest, n_src)\n",
    "    \"\"\"\n",
    "    # Create destination and source position indices\n",
    "    i = ops.arange(n_dest)[:, None]  # Shape: (n_dest, 1)\n",
    "    j = ops.arange(n_src)            # Shape: (n_src,)\n",
    "\n",
    "    # Compute lower triangular matrix (causal mask)\n",
    "    mask_matrix = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(mask_matrix, dtype)  # Convert boolean mask to specified dtype\n",
    "\n",
    "    # Reshape to add batch dimension\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "\n",
    "    # Tile the mask to match the batch size\n",
    "    mult = ops.concatenate([\n",
    "        ops.expand_dims(batch_size, -1),  # Shape: [1]\n",
    "        ops.convert_to_tensor([1, 1])     # Shape: [2]\n",
    "    ], axis=0)\n",
    "    \n",
    "    return ops.tile(mask, mult)  # Final shape: (batch_size, n_dest, n_src)\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    A single transformer decoder block implementing:\n",
    "    - Causal self-attention (no lookahead)\n",
    "    - Feedforward neural network (FFN)\n",
    "    - Residual connections\n",
    "    - Layer normalization\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, name=None):\n",
    "        \"\"\"\n",
    "        Initializes the transformer block.\n",
    "\n",
    "        Parameters:\n",
    "        - embed_dim (int): Dimension of the token embeddings.\n",
    "        - num_heads (int): Number of attention heads.\n",
    "        - ff_dim (int): Hidden dimension of the feedforward network.\n",
    "        - rate (float): Dropout rate.\n",
    "        - name (str): Optional name for the layer.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name) # Initialize the base Layer class\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),  # Position-wise feedforward\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Executes the forward pass of the transformer block.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs (tf.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim)\n",
    "        - training (bool): Whether the call is in training mode (enables dropout)\n",
    "\n",
    "        Returns:\n",
    "        - tf.Tensor: Output tensor of same shape as input\n",
    "        \"\"\"\n",
    "        # Extract batch size and sequence length\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "\n",
    "        # Generate causal mask to block attention to future tokens\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size=batch_size,\n",
    "            n_dest=seq_len,\n",
    "            n_src=seq_len,\n",
    "            dtype=\"bool\"\n",
    "        )\n",
    "        # Apply causal multi-head self-attention\n",
    "        attention_output = self.att(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            training=training\n",
    "        )\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attention_output)  # Residual + Norm\n",
    "\n",
    "        # Feedforward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)  # Residual + Norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "maxlen = 128\n",
    "vocab_size = 98308\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "ff_dim = 1024\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers, ops\n",
    "\n",
    "def create_model(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim):\n",
    "    \"\"\"\n",
    "    Builds and compiles a simple transformer-based language model.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embed_dim (int): Dimension of token and position embeddings.\n",
    "        num_heads (int): Number of attention heads in the transformer block.\n",
    "        feed_forward_dim (int): Dimension of the feed-forward network.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    # Input layer expecting integer token IDs\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\", name=\"input_tokens\")\n",
    "\n",
    "    # Token and position embeddings\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    # Transformer block with causal masking\n",
    "    # Stack Transformer blocks dynamically\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "    # Final dense layer maps to vocabulary size for language modeling\n",
    "    logits = layers.Dense(vocab_size, name=\"output_logits\")(x)\n",
    "\n",
    "    # Define model with both logits and intermediate embeddings as output (for optional use)\n",
    "    model = keras.Model(inputs=inputs, outputs=[logits, x], name=\"transformer_decoder\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set global random seed for Python, NumPy, and TensorFlow for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use for random number generators. Default is 42.\n",
    "\n",
    "    Usage:\n",
    "        >>> from utils.seed import set_seed\n",
    "        >>> set_seed(123)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Python built-in randomness seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set NumPy seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set TensorFlow randomness\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Keras-specific additional seed setting for deterministic initialization\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    # Enable full determinism in TensorFlow operations\n",
    "    try:\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "    except AttributeError:\n",
    "        print(\"[WARNING] `enable_op_determinism` not available in this TensorFlow version.\")\n",
    "\n",
    "    print(f\"[INFO] Global seed set to {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_callbacks(\n",
    "    base_dir: str = \"experiments\",\n",
    "    monitor: str = \"val_loss\",\n",
    "    model_name: str = \"transformer_decoder_model\"\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Creates standard Keras callbacks for training monitoring, checkpointing, and early stopping.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Directory where experiment logs and checkpoints are stored.\n",
    "        monitor (str): Metric to monitor for checkpointing, LR reduction, and early stopping.\n",
    "        model_name (str): Name of the model used in checkpoint filename.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tf.keras.callbacks.Callback instances.\n",
    "    \"\"\"\n",
    "    # Timestamp for experiment versioning\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    experiment_dir = os.path.join(base_dir, timestamp)\n",
    "\n",
    "    # Paths\n",
    "    log_dir = os.path.join(experiment_dir, \"logs\")\n",
    "    ckpt_path = os.path.join(experiment_dir, \"checkpoints\", f\"best_{model_name}.keras\")\n",
    "    csv_log_path = os.path.join(experiment_dir, \"metrics.csv\")\n",
    "\n",
    "    # Ensure directories exist\n",
    "    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Create callbacks\n",
    "    return [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=ckpt_path,\n",
    "            monitor=monitor,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.CSVLogger(\n",
    "            filename=csv_log_path,\n",
    "            append=False\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=monitor,\n",
    "            factor=0.1,\n",
    "            patience=3,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor,\n",
    "            patience=6,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from keras_nlp.metrics import Perplexity\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "def get_metrics(mask_token_id=0):\n",
    "    \"\"\"\n",
    "    Returns standard evaluation metrics for language modeling.\n",
    "\n",
    "    Args:\n",
    "        mask_token_id (int): Token ID to ignore during perplexity calculation.\n",
    "\n",
    "    Returns:\n",
    "        list: List of compiled metrics.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Perplexity(from_logits=True, mask_token_id=mask_token_id),\n",
    "        SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_fn: callable,\n",
    "        train_ds: tf.data.Dataset,\n",
    "        val_ds: tf.data.Dataset,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Trainer.\n",
    "\n",
    "        Args:\n",
    "            model_fn (Callable): Function that returns a Keras model instance.\n",
    "            train_ds (tf.data.Dataset): Prepared training dataset.\n",
    "            val_ds (tf.data.Dataset): Prepared validation dataset.\n",
    "            config (dict): Dictionary loaded from YAML config file.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.model = model_fn()           # Build the model\n",
    "        self._compile_model()             # Compile with optimizer, loss, metrics\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.callbacks = get_callbacks(   # Initialize callbacks\n",
    "            monitor=\"val_loss\")\n",
    "\n",
    "    def _compile_model(self):\n",
    "        \"\"\"\n",
    "        Compile the model using settings from the configuration.\n",
    "        Supports Adam and SGD optimizers with optional weight decay.\n",
    "        \"\"\"\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.01,\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.98,\n",
    "                weight_decay=0.01\n",
    "                )\n",
    "\n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=[loss_fn, None],\n",
    "            metrics=get_metrics()\n",
    "        )\n",
    "\n",
    "    def train(self) -> tf.keras.Model:\n",
    "        \"\"\"\n",
    "        Execute the model training loop.\n",
    "\n",
    "        Returns:\n",
    "            Trained Keras model.\n",
    "        \"\"\"\n",
    "        self.model.fit(\n",
    "            self.train_ds,\n",
    "            validation_data=self.val_ds,\n",
    "            epochs=5,\n",
    "            verbose=1,\n",
    "            callbacks=self.callbacks\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Transformer Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seed for full reproducibility\n",
    "set_seed = config.training[\"seed\"]\n",
    "\n",
    "train_ds, val_ds, test_ds = main()\n",
    "\n",
    "# Define model function that returns a compiled model\n",
    "def model_fn():\n",
    "    return create_model(\n",
    "        maxlen=config.model[\"max_sequence_length\"],\n",
    "        vocab_size=config.model[\"vocab_size\"],\n",
    "        embed_dim=config.model[\"embed_dim\"],\n",
    "        num_heads=config.model[\"num_heads\"],\n",
    "        feed_forward_dim=config.model[\"feed_forward_dim\"]\n",
    "    )\n",
    "\n",
    "# Initialize Trainer and start training\n",
    "trainer = Trainer(\n",
    "    model_fn=model_fn,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds\n",
    ")\n",
    "\n",
    "model = trainer.train()\n",
    "\n",
    "# Evaluate final model on validation and test datasets\n",
    "print(\"\\n✅ Evaluating model on validation set...\")\n",
    "val_loss, val_acc = model.evaluate(val_ds)\n",
    "print(f\"📊 Final Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n🧪 Evaluating model on test set...\")\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"🧪 Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save final trained model to disk\n",
    "os.makedirs(\"exports\", exist_ok=True)\n",
    "model_path = \"exports/transformer_decoder_model.keras\"\n",
    "model.save(model_path)\n",
    "print(f\"\\n✅ Final model saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
